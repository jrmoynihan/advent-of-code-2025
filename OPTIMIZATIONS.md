# Optimization Guide - Advent of Code 2025

Complete guide to all successful optimizations applied across Days 2, 7, 8, 9, and 10.

---

## Quick Reference

| Day | Technique | Speedup | Complexity | When to Use |
|-----|-----------|---------|------------|-------------|
| **8** | Vec Union-Find + Partial Sort | **9.52x** | Medium | Dense integer keys, need top K |
| **2** | Rayon Parallelization | **3.9x** | Low | Independent work units |
| **10** | Bit-packing (u32) | **2.69x** | Low | Boolean state ‚â§32 bits |
| **9** | Geo Crate | **1.90x** | Low | Complex geometry operations |
| **7** | Bit-packing (3√óu64) | **1.26x** | Medium | Boolean state ‚â§192 bits |

**Total time saved: ~150ms per full run**

---

## Day-by-Day Optimizations

### Day 8: Playground - 9.52x Speedup üèÜ

**Problem:** Minimum Spanning Tree with Union-Find  
**Before:** 60.0ms (Part 1), 58.4ms (Part 2)  
**After:** 6.3ms (Part 1), 47.2ms (Part 2)

#### Optimization 1: Integer Math (Trivial)
```rust
// Before: Expensive floating point
dx.powf(2.0) + dy.powf(2.0) + dz.powf(2.0)

// After: Integer arithmetic
(dx * dx + dy * dy + dz * dz) as f64
```
**Impact:** Small but free win

#### Optimization 2: Vec-based Union-Find (Huge Win)
```rust
// Before: HashMap with expensive lookups
struct UnionFind {
    parent: HashMap<JunctionBox, JunctionBox>,
    size: HashMap<JunctionBox, u64>,
}

// After: Direct array access
struct UnionFind {
    parent: Vec<usize>,
    size: Vec<usize>,
}
```
**Why it's faster:**
- HashMap: Hash calculation + collision handling + scattered memory
- Vec: Single array index + sequential memory + cache-friendly
- **10-20x faster lookups**

#### Optimization 3: Partial Sorting (Massive Win for Part 1)
```rust
// Before: Sort all 1,225 edges
all_edges.sort_by(...);  // O(N¬≤ log N¬≤)

// After: Partition around 10th element
all_edges.select_nth_unstable_by(10, ...);  // O(N¬≤)
all_edges[..10].sort_by(...);  // O(1)
```
**Impact:** O(n log n) ‚Üí O(n) for finding top K elements

**Results:**
- Part 1: 60.0ms ‚Üí 6.3ms (9.52x faster!)
- Part 2: 58.4ms ‚Üí 47.2ms (1.24x faster)

---

### Day 2: Gift Shop - 3.9x Speedup with Rayon

**Problem:** Independent range processing  
**Before:** 9.7ms (Part 1), 35.1ms (Part 2)  
**After:** 2.5ms (Part 1), 10.6ms (Part 2)

#### Rayon Parallelization
```rust
// Before: Sequential
ranges.for_each(|range| {
    for id in start..=end { /* check pattern */ }
});

// After: Parallel
use rayon::prelude::*;
ranges.par_iter().map(|range| {
    for id in start..=end { /* check pattern */ }
}).sum()
```

**Why it works:**
- Ranges are completely independent
- Substantial work per range
- No shared mutable state
- Rayon's work-stealing scheduler

**Results:**
- Part 1: 9.7ms ‚Üí 2.5ms (3.88x faster)
- Part 2: 35.1ms ‚Üí 10.6ms (3.31x faster)
- Nearly linear scaling with CPU cores

---

### Day 10: Factory - 2.69x Speedup with Bit-Packing

**Problem:** Boolean state machines with XOR operations  
**Before:** 11.4ms  
**After:** 4.2ms

#### Bit-Packed State (u32 bitmasks)
```rust
// Before: Vec<bool> with loops
let mut state = vec![false; num_lights];
for light_idx in 0..num_lights {
    state[light_idx] ^= buttons[button_idx][light_idx];
}
if state == target { /* match */ }

// After: Single XOR instruction
let mut state: u32 = 0;
state ^= button_mask;  // ‚Üê Single XOR!
if state == target_mask { /* match */ }
```

**Key optimizations:**
1. **Precompute button effects as u32 bitmasks** (one-time conversion)
2. **Single XOR operation** instead of loop (hardware-accelerated)
3. **Direct integer comparison** instead of Vec iteration
4. **Stack allocation** (no heap, better cache)

**Results:**
- 11.4ms ‚Üí 4.2ms (2.69x faster)
- Per machine: 61.5¬µs ‚Üí 22.9¬µs

**Why it works:**
- Small state (4-10 bits)
- Millions of state transitions
- XOR is perfect for bitwise operations
- Bulk operations (XOR all bits, compare all bits)

---

### Day 9: Movie Theater - 1.90x Speedup with Geo Crate

**Problem:** Point-in-polygon for 60k cells with 496-vertex polygon  
**Before:** 98.7ms  
**After:** 52.0ms

#### Professional Library (Geo Crate)
```rust
// Before: Custom implementation
fn point_in_polygon(p: Point, polygon: &[Point]) -> bool {
    // Single pass through 496 edges
    // Ray casting + boundary check
    // ~1.3¬µs per call
}

// After: Professional library
use geo::{Contains, Polygon};
polygon.contains(&point)  // ~0.5¬µs per call (2.6x faster!)
```

**Why Geo Crate Wins:**
- Optimized winding number algorithm
- SIMD-friendly operations
- Better numerical handling
- Years of production optimization

**Results:**
- 98.7ms ‚Üí 52.0ms (1.90x faster)
- Saved 46.7ms
- Less code to maintain
- More robust

**When to use libraries:**
- ‚úÖ Well-established domain (geometry, crypto, etc.)
- ‚úÖ Performance-critical operations
- ‚úÖ Complex algorithms with edge cases
- ‚úÖ Would take days/weeks to implement correctly

---

### Day 7: Laboratories - 1.26x Speedup + 85% Memory Reduction

**Problem:** Boolean beam positions (141 wide)  
**Before:** 23.5¬µs, 165 bytes  
**After:** 18.7¬µs, 24 bytes

#### Bit-Packed State (3√óu64 chunked)
```rust
// Before: Vec<bool> on heap (141 bytes + 24 metadata)
let mut beams = vec![false; 141];

// After: 3 u64s on stack (24 bytes total)
struct BeamState {
    chunks: [u64; 3],  // Fits in L1 cache
}

impl BeamState {
    #[inline]
    fn get(&self, pos: usize) -> bool {
        let chunk_idx = pos / 64;
        let bit_idx = pos % 64;
        (self.chunks[chunk_idx] & (1u64 << bit_idx)) != 0
    }
}
```

**Results:**
- 23.5¬µs ‚Üí 18.7¬µs (1.26x faster)
- 165 bytes ‚Üí 24 bytes (85.5% memory reduction)
- Entire state fits in single cache line (64 bytes)

**Why it works:**
- Stack allocation (no heap)
- Better cache utilization
- Sequential memory access
- Guaranteed cache hit

---

## Optimization Techniques

### 1. Bit-Packing (Days 7, 10)

**When to use:**
- Boolean/binary state
- Small state space (‚â§64 for single u64, ‚â§192 for 3 u64s)
- Frequent state transitions
- XOR/toggle operations (GF(2) systems)

**Performance characteristics:**

| Operation | Vec<bool> | Bit-packed (u32) | Bit-packed (3√óu64) |
|-----------|-----------|------------------|--------------------|
| Single bit check | 1-2 cycles | 3-4 cycles | 4-5 cycles |
| XOR all bits (n=8) | 8-16 cycles | **1 cycle** ‚ö° | **3 cycles** ‚ö° |
| Compare all bits | n cycles | **1 cycle** ‚ö° | **3 cycles** ‚ö° |
| Memory per bit | 8 bytes | 1 bit | 1 bit |

**Implementation patterns:**

```rust
// Pattern 1: Single Register (‚â§64 bits)
let mut state: u64 = 0;
state |= 1 << pos;           // Set bit
state &= !(1 << pos);         // Clear bit
let is_set = (state & (1 << pos)) != 0;  // Check bit

// Pattern 2: Multiple Registers (64-192 bits)
struct State {
    chunks: [u64; 3],
}
fn get(&self, pos: usize) -> bool {
    let chunk = pos / 64;
    let bit = pos % 64;
    (self.chunks[chunk] & (1 << bit)) != 0
}

// Pattern 3: Precomputed Masks
let button_mask = button_to_mask(&button_effects);  // Precompute once
state ^= button_mask;  // Use many times
```

---

### 2. Data Structure Selection (Day 8)

**HashMap ‚Üí Vec for dense integer keys:**

```rust
// When keys are 0..n, use Vec instead of HashMap
// HashMap: Hash + probe + compare + scattered memory
// Vec: Direct indexing + sequential memory + cache-friendly

// Example: Union-Find
struct UnionFind {
    parent: Vec<usize>,  // Instead of HashMap<usize, usize>
    size: Vec<usize>,
}
```

**Impact:** 5-10x faster lookups

**When to use:**
- ‚úÖ Dense integer keys (0..n)
- ‚úÖ Frequent lookups
- ‚úÖ Cache performance matters

**When HashMap is better:**
- Sparse keys
- Non-integer keys
- Infrequent lookups

---

### 3. Partial Sorting (Day 8)

**When you only need top K:**

```rust
// Before: Full sort
all_items.sort_by(...);  // O(n log n)
take(k)

// After: Partial sort
all_items.select_nth_unstable_by(k, ...);  // O(n) average
all_items[..k].sort_by(...);  // O(k log k) ‚âà O(1)
```

**Impact:** O(n log n) ‚Üí O(n) for finding extremes

**When to use:**
- ‚úÖ Only need top K elements
- ‚úÖ Don't need full ordering
- ‚úÖ K << N

---

### 4. Parallelization with Rayon (Day 2)

**When work is independent:**

```rust
use rayon::prelude::*;

// Change iter() to par_iter()
items.par_iter().map(|item| {
    // Independent work
}).sum()
```

**Impact:** ~4x on quad-core CPUs

**Requirements:**
- ‚úÖ Independent work units
- ‚úÖ >10¬µs work per item (to overcome thread overhead)
- ‚úÖ No shared mutable state

**Why it works:**
- Work-stealing scheduler
- Automatic thread pool management
- Zero-overhead when work is small
- Scales with available cores

---

### 5. Professional Libraries (Day 9)

**When to use external crates:**

```rust
// Example: Geo crate for computational geometry
use geo::{Contains, Polygon};

let polygon = GeoPolygon::new(...);
if polygon.contains(&point) {
    // Optimized implementation
}
```

**Impact:** 1.7-3x over naive custom implementations

**When to use:**
- ‚úÖ Well-established domain (geometry, crypto, linear algebra)
- ‚úÖ Performance-critical operations
- ‚úÖ Complex algorithms with edge cases
- ‚úÖ Would take days/weeks to implement correctly

**Popular crates:**
- `geo` - Computational geometry
- `rayon` - Parallelization
- `petgraph` - Graph algorithms
- `ndarray` - N-dimensional arrays

---

## Optimization Decision Tree

```
Is it slow? (>100ms)
‚îú‚îÄ Yes ‚Üí Profile to find bottleneck
‚îÇ  ‚îî‚îÄ Bottleneck is...
‚îÇ     ‚îú‚îÄ Boolean state operations ‚Üí Try bit-packing
‚îÇ     ‚îú‚îÄ HashMap lookups ‚Üí Try Vec if keys are dense
‚îÇ     ‚îú‚îÄ Sorting ‚Üí Try partial sort if you need top K
‚îÇ     ‚îú‚îÄ Independent work ‚Üí Try Rayon
‚îÇ     ‚îî‚îÄ Complex algorithm ‚Üí Try professional library
‚îî‚îÄ No ‚Üí Don't optimize, good enough!
```

---

## Quick Optimization Checklist

### Quick Wins (Try These First)
- [ ] Replace `Vec<bool>` with bitmasks (if ‚â§64 bits)
- [ ] Use `Vec` instead of `HashMap` for dense 0..n indices
- [ ] Add `.par_iter()` if work is independent (>10¬µs per item)
- [ ] Replace expensive math (`powf`, `sqrt`) with integers
- [ ] Use partial sort (`select_nth_unstable`) if finding top K

### Medium Wins
- [ ] Profile to find 80%+ bottleneck
- [ ] Extract hot paths to `#[inline]` functions
- [ ] Eliminate redundant loops (combine passes)
- [ ] Use professional libraries for complex algorithms
- [ ] Precompute when reused >100 times

### Advanced (Measure First!)
- [ ] SIMD for bulk operations
- [ ] Custom data structures
- [ ] Algorithm redesign (sweep line, etc.)
- [ ] Memory layout optimization

### Don't Bother
- ‚ùå Micro-optimizations on <1% of runtime
- ‚ùå Parallelizing <10¬µs work items
- ‚ùå Sorting when you'll check most items anyway
- ‚ùå Caching with <20% hit rate

---

## Performance Summary

### Before Optimization
```
Day 2:  22.4ms
Day 7:   0.06ms
Day 8:   59.2ms
Day 9:   98.7ms
Day 10:  11.4ms
Total: ~191ms
```

### After Optimization
```
Day 2:   6.5ms  (3.9x faster)
Day 7:   0.05ms (1.26x faster)
Day 8:  27ms    (2.2x faster)
Day 9:  52ms    (1.9x faster)
Day 10:  4.2ms  (2.7x faster)
Total: ~90ms
```

**Overall speedup: 2.12x**  
**Time saved: ~100ms per full run**

---

## Key Learnings

### 1. Profile First, Optimize Second
- Day 9 profiling revealed 92% time in one function
- Optimizing the 0.5% bottleneck would be waste
- **Tool:** Manual timing with `Instant::now()`

### 2. Data Structure Choice Matters Most
- Vec vs HashMap: 10x difference for dense indices
- Structure choice > micro-optimizations

### 3. Low-Hanging Fruit First
- Rayon: Change `iter()` ‚Üí `par_iter()` for 4x speedup
- Integer math: Remove `powf()` for free win
- Professional libraries: Often faster than custom

### 4. Complexity Isn't Always Better
- Simple bit-packing: 2.7x speedup
- Complex parallel + sorting: Can be slower
- **KISS principle applies to optimization**

### 5. Know Your Tools
- `select_nth_unstable`: O(n) vs O(n log n)
- Rayon: Free parallelism for independent work
- Geo crate: Years of optimization work

---

## Tools & Crates

### Built-in Rust
- `rayon`: Parallelization
- `std::simd`: SIMD operations

### External Crates
- `geo`: Computational geometry (Day 9)
- `rstar`: R-tree spatial index (Day 9, for comparison)

### Performance Tools
- `std::time::Instant`: Manual benchmarking
- `cargo test --release`: For accurate measurements
- `#[inline]`: Compiler hints for hot paths

---

## For Future Advent of Code Years

### Start With
1. Get correct solution first
2. Profile to find slowest days
3. Apply techniques from this guide

### Quick Reference Table

| Problem Type | First Try | If That Fails |
|--------------|-----------|---------------|
| Boolean state | Bit-packing | SIMD |
| Graph with indices | Vec-based | HashMap if sparse |
| Need top K | Partial sort | Heap |
| Independent work | Rayon | Multi-threading |
| Complex algorithm | Find library | Optimize custom |
| Geometry | geo crate | Custom + SIMD |

### Crates to Keep Handy
- `rayon` - Parallelization
- `geo` - Computational geometry
- `ndarray` - N-dimensional arrays
- `petgraph` - Graph algorithms

---

## Conclusion

### What We Accomplished
- ‚úÖ Optimized 5 days (2, 7, 8, 9, 10)
- ‚úÖ Total speedup: 2.12x overall
- ‚úÖ Time saved: ~100ms per run
- ‚úÖ Comprehensive documentation

### Most Important Lessons
1. **Profile before optimizing** - Don't guess bottlenecks
2. **One change at a time** - Measure each change
3. **Use libraries** - Don't reinvent complex algorithms
4. **Know when to stop** - <100ms is usually good enough

### Best Practices Established
- Bit-packing for boolean state
- Vec for dense indices
- Rayon for independent work
- Professional libraries for complex domains
- Always benchmark before and after

